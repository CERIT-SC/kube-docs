# Spuštění Virtuálního Stroje

## Požadavky

1. Navštivte [https://rancher.cloud.e-infra.cz](https://rancher.cloud.e-infra.cz) a zapamatujte si svůj [namespace](../rancher/quotas).
2. Nakonfigurujte nástroj `kubectl`, viz sekce [kubectl](../kubernetes/kubectl).
3. Počítač s nainstalovaným příkazem `ssh`.

## Spuštění Jednoduchého Virtuálního Stroje

Spuštění skutečného virtuálního stroje není samozřejmě možné v kontejnerizované infrastruktuře. Následující kroky však ukazují, jak spustit něco velmi blízkého virtuálnímu stroji.

Spuštění VM se skládá z několika kroků:

1. Vygenerujte [ssh klíč](https://www.ssh.com/academy/ssh/key)
2. Vytvořte `secret` s veřejným ssh klíčem. 
3. Vyberte docker obraz kompatibilní s ssh
4. Vytvořte a spusťte manifest

### SSH Klíče

V následujícím preferujte typ klíče RSA pro maximální kompatibilitu.

#### Vygenerujte SSH klíč -- Linux/MacOS

Ssh klíče se obvykle nacházejí v domovském adresáři v podadresáři `.ssh` a jsou pojmenovány jako `id_rsa.pub`. Pokud takový adresář nebo soubory neexistují, můžete vygenerovat nové klíče pomocí příkazu `ssh-keygen`, který vygeneruje tajný a veřejný klíč. Názvy souborů se vytisknou během generování klíče, např.:
```
$ ssh-keygen 
Generating public/private rsa key pair.
Enter file in which to save the key (/home/user/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/user/.ssh/id_rsa.
Your public key has been saved in /home/user/.ssh/id_rsa.pub.
```
V tomto případě je `id_rsa` soukromý klíč (uchovávejte ho v tajnosti a nikdy ho neposílejte), `id_rsa.pub` je veřejný klíč. Heslo není povinné, ale doporučuje se.

#### Vygenerujte SSH klíč -- Microsoft Windows

V tomto případě postupujte podle [návodu zde](https://www.puttygen.com/).

#### Vytvoření Secret

Pro tento krok je třeba mít nainstalovaný a nakonfigurovaný `kubectl`. Vytvořte secret vydáním:
```
kubectl create secret generic ssh-publickey --from-file=ssh-publickey=~/.ssh/id_rsa.pub -n [namespace]
```
Nahraďte `[namespace]` názvem vašeho *namespace* z Rancheru. Pokud jste vygenerovali nebo používáte jiný typ klíče než *RSA*, nahraďte `id_rsa.pub` správnou cestou k veřejnému klíči.

### Docker Obraz

Vytvořili jsme 4 základní obrazy pro veřejné použití:
1. `cerit.io/pub/ssh-base:d10` -- obraz založený na Debianu 10 (Buster)
2. `cerit.io/pub/ssh-base:d11` -- obraz založený na Debianu 11 (Bullseye)
3. `cerit.io/pub/ssh-base:ubuntu20.04` -- obraz založený na Ubuntu 20.04 (Focal)
4. `cerit.io/pub/ssh-base:ubuntu22.04` -- obraz založený na Ubuntu 22.04 (Jellyfish)

Tyto obrazy lze použít přímo nebo jako základní obrazy pro vytváření vlastních pokročilejších obrazů, viz níže.

### Manifest

[Stáhněte](/examples/ceritsc/manifests/vm-simple.yaml) manifest. Upravte řádek 6:

```yaml
annotations:
  external-dns.alpha.kubernetes.io/hostname: vm-[namespace].dyn.cloud.e-infra.cz
```

a nahraďte `[namespace]` vaším *namespace*. Tento *namespace* musí být stejný jako *namespace* použitý pro secret.

Pokud si přejete, nahraďte název `image` na řádku 34:
```yaml
image: cerit.io/pub/ssh-base:d10
```
jakýmkoli jiným obrazem uvedeným výše, například `cerit.io/pub/ssh-base:ubuntu22.04`. Uložte soubor a spusťte manifest:
```
kubectl create -f vm-simple.yaml -n [namespace]
```
Předpokládáme, že stažený a upravený soubor má název `vm-simple.yaml`. Znovu nahraďte `[namespace]` vaším *namespace* z Rancheru. Tento příkaz spustí manifest. Můžete zkontrolovat `Workload` -> `Pods` v Rancheru, abyste viděli, že váš manifest běží:
![vm-ssh-simple](/img/ceritsc/vm-ssh.png)

Pokud je potřeba GPU, požádejte o GPU v sekci `limits`:
```
resources:
  limits:
    cpu: "1"
    memory: "4Gi"
    nvidia.com/gpu: "1"
```

### Přihlášení

Pokud manifest běží, můžete se přihlásit pomocí příkazu ssh. Přihlašovací jméno je vždy `user`, nemá být nahrazeno.
```
ssh user@vm-[namespace].dyn.cloud.e-infra.cz
```
Opět nahraďte `[namespace]` vaším *namespace* z Rancheru a měli byste vidět něco jako toto: 
```
anubis: ~ $ ssh user@vm-hejtmanek1-ns.dyn.cloud.e-infra.cz
Warning: Permanently added 'vm-hejtmanek1-ns.dyn.cloud.e-infra.cz' (ED25519) to the list of known hosts.
X11 forwarding request failed on channel 0

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
user@vm-example-0:~$ 
```

V tomto okamžiku máte spuštěný svůj VM pro obecné účely.

## Smazání Běžícího VM

Pokud váš VM již není potřeba, laskavě žádáme o jeho smazání. Může být smazán vydáním:
```
kubectl delete -f vm-simple.yaml -n [namespace]
```
Kde `vm-simple.yaml` je soubor použitý pro vytvoření a `[namespace]` je váš *namespace* z Rancheru.

## Instalace Dalšího Softwaru

Existují dvě možnosti, jak nainstalovat další balíčky do VM. Můžete buď přebudovat docker obraz, nebo můžete použít existující, nainstalovat správce balíčků `conda` pro další instalaci balíčků. Pomocí dockeru můžete nainstalovat všechny standardní balíčky z základního systému, tj. Debian nebo Ubuntu, tyto balíčky budou součástí nového obrazu a budou vždy (i po restartu kontejneru) k dispozici. 

**Poznámka**: Nemůžete nainstalovat žádný systémový balíček v běžícím kontejneru.

Nicméně, pomocí `conda` je instalace balíčků `conda` možná i v běžícím kontejneru. Viz upozornění níže.

### Přebudování Obrazu

Pokud nejste obeznámeni s docker build. Zkontrolujte naši [dokumentaci](../docker/dockerfile). Pro docker obraz je potřeba docker registry. Můžete použít naši [https://hub.cerit.io](https://hub.cerit.io) registraci, která může uchovávat váš docker obraz. Obrazy mohou být označeny jako: `cerit.io/[project]/[image]:[tag]`. Viz [dokumenty](../docker/harbor).

Pro přebudování jednoho z výše uvedených obrazů použijte následující příklad `Dockerfile`:
```
FROM cerit.io/pub/ssh-base:d10

RUN apt-get update && apt-get -t install vim less && apt-get clean && rm -rf /var/lib/apt/lists/
```

Tento `Dockerfile` vytváří novou verzi docker obrazu s nainstalovanými balíčky `vim` a `less`. Uložte příklad do souboru `Dockerfile`, změňte seznam nainstalovaných balíčků podle potřeby. Docker obraz můžete vytvořit pomocí:
```
docker build -t cerit.io/[login]/[image]:[tag] - < Dockerfile
```
Nahraďte `[login]` názvem našeho *projektu* v `hub.cerit.io`, `[image]:[tag]` názvem a tagem obrazu.

Aby bylo možné obraz uložit do registru, musíte se nejprve přihlásit do registru pomocí:
```
docker login cerit.io
```
s přihlašovacími údaji, které můžete získat na [https://hub.cerit.io](https://hub.cerit.io).

Po přihlášení můžete *pushnout* svůj nový obraz pomocí:
```
docker push cerit.io/[login]/[image]:[tag]
```

Nahraďte `image` v manifestu výše tímto novým názvem `cerit.io/[login]/[image]:[tag]` a odstraňte a spusťte manifest znovu.

### Správce balíčků Conda

Pomocí [conda](https://docs.conda.io/en/latest/) nebo nástroje `mamba` můžete instalovat nové balíčky i v běžícím kontejneru. Nejprve nainstalujte `conda/mamba` zadáním jeden po druhém následujících příkazů:
```
wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh
/bin/bash Mambaforge-Linux-x86_64.sh -f -b -p /home/user/conda
/home/user/conda/bin/conda init
echo '. ~/.bashrc' >> ~/.profile
echo 'export LD_LIBRARY_PATH=/home/user/conda/lib' >> ~/.bashrc
```

Odhlaste se a přihlaste se znovu. Nyní byste měli vidět výzvu jako je tato:
```
(base) user@vm-example-0:~$
```

Nyní jste připraveni instalovat balíčky, např. balíček `mc`:
```
(base) user@vm-example-0:~$ mamba install mc
```

Po chvíli `mamba` dokončí a vy můžete používat nainstalovaný balíček `mc`.

Všechny balíčky jsou nainstalovány do adresáře `/home/user/conda`.

#### Upozornění

* V tomto jednoduchém případě obsah `/home/user/conda` není zachován mezi restartováním kontejneru. Musíte nakonfigurovat trvalý domov, viz níže.
* Velikost celého kontejneru je omezena na `4GB`. To je nastaveno pomocí: `ephemeral-storage: "4Gi"`, pokud je to potřeba, tuto hodnotu lze zvýšit. Pokud je překročena velikost `4GB`, kontejner bude *vysídlen* a restartován.

## Trvalý domov

Jak bylo zmíněno výše, disky uvnitř kontejneru nejsou trvalé. To znamená, že vše, co je nainstalováno pomocí `conda`/`mamba`, je ztraceno, pokud je kontejner restartován nebo znovu vytvořen. Abychom to vyřešili, je třeba vytvořit trvalý domov.

Můžete stáhnout [manifest](/examples/ceritsc/manifests/vm-persistent.yaml), který obsahuje definici trvalého domova. Trvalý domov je propojen s názvem nasazení a jeho verzí. Název je v manifestu:

```yaml
kind: StatefulSet
metadata:
  name: vm-pvc-example
```

Výchozí verze je `-0`, takže v tomto případě bude trvalý domov propojen s `vm-pvc-example-0`. Odpovídající [PVC](../kubernetes/pvc) se nazývá `home-vm-pvc-example-0`. Tento PVC můžete najít v Rancheru pod `Storage` -> `PersistentVolumeClaims`. To je použitelné pro případ, kdy je obsah domova poškozen a je třeba jej smazat. Můžete smazat PVC z tohoto uživatelského rozhraní Rancher a začít znovu s prázdným.

Musíte změnit stejné položky v tomto manifestu jako v *jednoduchém* případě, tj. `external-dns.alpha.kubernetes.io/hostname` a `image`. Spustíte to stejným způsobem jako v *jednoduchém* případě pomocí 

```shell
kubectl create -f vm-persistent.yaml -n [namespace]
```

## Zdroje

Oba výše uvedené příklady požadují **1 CPU**, **4GB paměti** a **4GB disku**. Tyto limity můžete změnit úpravou následující části manifestu:

```yaml
limits:
  cpu: "1"
  memory: "4Gi"
  ephemeral-storage: "4Gi"
```

**Poznámky:**

1. Počet CPU je nastaven jako číslo, například `1`, nebo zlomek, například `100m`, což znamená *0.1 CPU*. Pro paměť a disk se používají jednotky `Mi` nebo `Gi`, což znamená Mega Bytes a Giga Bytes, resp.

2. Pro `conda`/`mamba` je vyžadováno alespoň 4GB paměti, jinak obdržíte zprávu `killed`, když se pokusíte spustit příkaz `conda`.

## Práce s GPU

Pro práci s GPU musí sekce zdrojů manifestu obsahovat požadavek na GPU. Můžete [stáhnout](/examples/ceritsc/manifests/vm-persistent-gpu.yaml) manifest nebo použít svůj vlastní s následujícími dodatky:

```yaml
limits:
  cpu: "1"
  memory: "4Gi"
  ephemeral-storage: "4Gi"
  nvidia.com/gpu: 1
```

Pokud spustíte tento manifest, ovladače NVIDIA a příkazy `nvidia-smi` budou k dispozici v kontejneru:

```shell
user@vm-pvc-example-0:~$ nvidia-smi
Wed May 25 17:53:50 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A40          On   | 00000000:A3:00.0 Off |                    0 |
|  0%   34C    P8    31W / 300W |      0MiB / 45634MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
user@vm-pvc-example-0:~$ 
```

Nicméně, *CUDA* nebo *Tensorflow* nebo *Pytorch* frameworky je třeba nainstalovat samostatně.

### CUDA

*CUDA* může být nainstalována buď úpravou běžícího kontejneru pomocí Dockerfile, jak bylo zmíněno výše, nebo pomocí `conda`/`mamba`. V druhém případě je potřeba alespoň **20GB** disku (řádek `ephemeral-storage`).

Pomocí `mamba` je instalace možná pomocí:
```
mamba install cudatoolkit-dev=11.4.0 cudatoolkit=11.4.2
```

Po úspěšné instalaci můžete zkontrolovat, zda CUDA funguje, zadáním:
```
/home/user/conda/pkgs/cuda-toolkit/extras/demo_suite/deviceQuery
```

Výstup by měl vypadat takto:
```
/home/user/conda/pkgs/cuda-toolkit/extras/demo_suite/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: "NVIDIA A40"
  CUDA Driver Version / Runtime Version          11.4 / 11.4
  CUDA Capability Major/Minor version number:    8.6
  Total amount of global memory:                 45634 MBytes (47850782720 bytes)
  (84) Multiprocessors, (128) CUDA Cores/MP:     10752 CUDA Cores
  GPU Max Clock rate:                            1740 MHz (1.74 GHz)
  Memory Clock rate:                             7251 Mhz
  Memory Bus Width:                              384-bit
  L2 Cache Size:                                 6291456 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  1536
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 39 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.4, CUDA Runtime Version = 11.4, NumDevs = 1, Device0 = NVIDIA A40
Result = PASS
```

**Poznámka:**

1. Nainstalujte verzi CUDA co nejblíže verzi zobrazené pomocí `nvidia-smi`, aktuálně je to verze **11.4.2**. Použijte `mamba search cuda` pro zobrazení dostupných verzí.

2. Pokud obdržíte chybu:
```
stderr: ./cuda-installer: error while loading shared libraries: libxml2.so.2: cannot open shared object file: No such file or directory
```
Chybí vám správné nastavení `conda`, zejména chybí `echo 'export LD_LIBRARY_PATH=/home/user/conda/lib' >> ~/.bashrc`.

### SHM

Pro mnoho aplikací GPU je vyžadována zvýšená sdílená paměť (SHM). Výchozí velikost sdílené paměti je 64 kB pro kontejnery. Zvýšení SHM se provádí připojením dalšího svazku do `/dev/shm`. Můžete si stáhnout příklad [manifestu](/examples/ceritsc/manifests/vm-persistent-gpu-shm.yaml).

Přidané sekce jsou:

1. Pod `volumeMounts`:
```yaml
- name: shm
  mountPath: /dev/shm
```

2. Celá nová sekce `volumes`:
```yaml
volumes:
- name: shm
  emptyDir:
    medium: Memory
    sizeLimit: 1Gi
```
Odsazení této sekce musí odpovídat řádku `containers`, to je důležité!

Tyto dvě sekce přidávají 1 GB sdílené paměti, `sizeLimit` označuje velikost SHM.

**Poznámka:** `sizeLimit` SHM se používá z limitů `memory` v sekcích `resources`, což znamená, že pokud je `memory` nastavena na 5 GB a `sizeLimit` na 1 GB, pak je k dispozici 4 GB paměti pro aplikace. Dotazování velikosti SHM pomocí příkazu `df` je zavádějící, nezobrazuje skutečnou velikost SHM.
