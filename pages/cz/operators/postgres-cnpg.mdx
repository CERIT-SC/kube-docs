# Postgres Operator Cloudnative-PG

[Postgres Operator](https://cloudnative-pg.io/documentation/1.23/) poskytuje snadné spuštění PostgreSQL clusteru na Kubernetes. Poskytli jsme clusterový Postgres Operator pro snadné nasazení serverů databáze Postgres SQL. Postgres Operator definuje typ `Cluster`, který zajišťuje existenci databáze nebo databázového clusteru. Pro vaše pohodlí poskytujeme několik funkčních příkladů v následujících sekcích spolu s některými pokročilými funkcemi.

Originální, úplná dokumentace o struktuře PostgresOperator je k dispozici [na jeho webových stránkách](https://cloudnative-pg.io/documentation/1.23/) s oficiálním manuálem pro vytvoření minimálního PostgreSQL clusteru umístěným v [sekci rychlého startu](https://cloudnative-pg.io/documentation/1.23/quickstart/). Pokud hledáte pokročilou konfiguraci nebo popis a vysvětlení YAML polí, konzultujte oficiální dokumentaci.


## Nasazení instance clusteru

Je možné nasadit buď jednotlivou, nebo clusterovou instanci. Pro testovací účely doporučujeme nasadit minimální (jednotlivou) instanci, která používá NFS úložiště jako podkladové úložiště a spotřebovává omezené zdroje (výkon je nízký). Pro lepší dostupnost a výkon lze použít nasazení clusteru. V tomto případě běží v clusteru více instancí, kde jedna z nich je vůdce a ostatní následují a synchronizují data od vůdce.

### Jednotlivá instance

V tomto příkladu je vytvořena jednotlivá instance s vygenerovaným uživatelským jménem a heslem. Je možné poskytnout vlastní objekt `Secret` místo automaticky vygenerovaného uživatelského jména/hesla - viz sekce **Vlastní přihlašovací údaje databáze**.

```yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: test-cluster       # name, can be modified
spec:
  instances: 1             # single instance

  imageName: 'cerit.io/cloudnative-pg/postgresql:15.0'  # image to use

  primaryUpdateStrategy: unsupervised

  bootstrap:
   initdb:
     database: mydb        # db to create, can be modified
     owner: myowner        # user to create, can be modified

  storage:
    size: 10Gi
    storageClass: nfs-csi  # storage class to use, can be modified
```

Spusťte `kubectl create -f minimal-cn.yaml -n [namespace]`, měli byste vidět pod s názvem `test-cluster-1` (pokud `metadata.name` nebylo změněno z příkladu) běžící ve vašem namespace.

Minimální manifest lze stáhnout [zde](/examples/ceritsc/manifests/minimal-cn.yaml).

### Instance clusteru

Pro nasazení verze clusteru můžete stáhnout [manifest clusteru](/examples/ceritsc/manifests/cluster-cn.yaml). Jediný rozdíl je v řádku určujícím počet instancí:

```yaml
numberOfInstances: 3
```

To požaduje 3 uzlový cluster. Tento typ nastavení je odolný vůči selhání uzlu --- pokud uzel běžící tuto instanci selže, databáze je znovu vytvořena na jiném uzlu, data jsou znovu připojena z NFS úložiště a operace pokračují. Na druhou stranu, toto nastavení není rychlostně nadřazené, i když jsou přidány zdroje.

*Poznámka*: Instance clusteru spotřebovávají více zdrojů a musíte zvážit, kolik zdrojů je k dispozici ve vašem namespace/projektu. Instance clusteru spotřebovává `numberOfInstances * limits` zdrojů.


## Přístup k databázi

Pro přístup k databázi musíte znát přihlašovací údaje a umístění databáze. 

### Přihlašovací údaje databáze

Pokud jste použili vlastní objekt `Secret`, použijte tyto přihlašovací údaje. Jinak zadejte příkaz (`test-cluster-app`, pokud `metadata.name` nebylo změněno z příkladu, jinak `[metadata-name]-app`)
```shell
kubectl get secret test-cluster-app -n [namespace] -o 'jsonpath={.data.pgpass}' | base64 -d
```

Zobrazí se řetězec ve formátu: `dbhost:port:dbname:dbuser:password`. Například:
```
test-cluster-rw:5432:mydb:myowner:uLUmkvAMwR0lJtw5ksUVKihd5OvCrD28RFH1JTLbbzO6BhEAtnWJr1L0nWpTi3GL
```

Použijte `dbuser:password` jako přihlašovací údaje pro připojení. 

### Přístup k databázi 

K databázi lze přistupovat (připojit se) ze stejného namespace, kde je nasazena, z jiného namespace nebo zvenčí Kubernetes clusteru. Databáze obvykle používají připojení pouze pro čtení (obvykle cílení na repliky) a připojení pro čtení a zápis (cílení na hlavní instance).

#### Přístup ze stejného namespace

Pokud byl cluster vytvořen s názvem `test-cluster`:

```yaml
kind: Cluster
metadata:
  name: test-cluster
...
```

Hostname pro použití repliky pouze pro čtení je `test-cluster-ro` a hostname zapisovatelného hlavního uzlu je `test-cluster-rw`. Pokud není výslovně nastaveno jinak, port je dobře známý postgres port `5432`.

#### Přístup z jiného namespace

Předpokládáme, že název databáze je `test-cluster`, hostname pouze pro čtení je `test-cluster-ro.[namespace].svc.cluster.local`, a zapisovatelný hlavní uzel je `test-cluster-rw.[namespace].svc.cluster.local`, nahraďte `[namespace]` názvem namespace, kde je databáze nasazena. 

Pokud není výslovně nastaveno jinak, port je dobře známý postgres port `5432`.

#### Přístup zvenčí Kubernetes clusteru

Pro přístup k databázi z externího světa (mimo Kubernetes cluster) musí být vytvořen nový objekt služby typu `LoadBalancer`. 

Pokud chcete také přístup pouze pro čtení (kromě přístupu pro zápis), musí být vytvořeny dva LoadBalancery. V tomto případě nás prosím kontaktujte na **k8s@ics.muni.cz**.

Existuje mírný rozdíl v vytvořených objektech v závislosti na tom, zda přistupujete k databázi **z Masarykovy univerzity/Masarykovy univerzity VPN** nebo **odkudkoli jinde**.

##### MU/MU VPN

Předpokládáme, že název databáze je opět `test-cluster`, následující příklady ukazují potřebné služby pro přístup jak pro čtení, tak pro zápis.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: test-cluster-lb-rw
  annotations:
    metallb.universe.tf/address-pool: privmuni
spec:
  type: LoadBalancer
  ports:
  - port: 5432
    targetPort: 5432
  selector:
    cnpg.io/cluster: test-cluster # if name was changed provide [cluster-name]
    role: primary
```

Anotace `metallb.universe.tf/allow-shared-ip` zajišťuje, že oba LoadBalancery sdílejí stejnou IP adresu a jsou rozlišovány podle portů:
- port `5433` je pro repliky pouze pro čtení
- port `5432` je pro zapisovatelné repliky

Tento příklad přiřazuje IP adresy, které jsou dostupné **pouze z interní sítě Masarykovy univerzity nebo prostřednictvím VPN služby Masarykovy univerzity**. 

##### Non-MU accesses

Předpokládáme, že název databáze je opět `test-cluster`, následující příklady ukazují potřebné služby pro přístup jak pro čtení, tak pro zápis.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: test-cluster-lb-rw
  annotations:
    metallb.universe.tf/address-pool: default
spec:
  type: LoadBalancer
  ports:
  - port: 5432
    targetPort: 5432
  selector:
    cnpg.io/cluster: test-cluster # if name was changed provide [cluster-name]
    role: primary
```

V tomto případě jsou IP adresy přidělovány z veřejného IP poolu. Při přístupu k databázi z externího světa **silně doporučujeme** nastavit síťovou politiku.

##### Retrieve Public IP to Connect

Po vytvoření této služby je databáze dostupná na veřejné IP adrese, kterou lze nalézt pomocí příkazu:
```shell
kubectl get svc [name_of_loadbalancer_service_from_previous_step] -n [namespace_where_cluster_is_deployed]
```

Výstup bude podobný následujícímu, kde `EXTERNAL-IP` označuje IP pro připojení (hostname/adresa v pgAdmin) a `PORT` port pro připojení.
```shell
kubectl get svc test-cluster-external -n test-postgres-ns          
NAME                    TYPE           CLUSTER-IP    EXTERNAL-IP       PORT(S)    AGE
test-cluster-external   LoadBalancer   10.43...      147.251.X.Y       5432/TCP   21h
```

### Network Policy

Pro zvýšení bezpečnosti se používá *síťová politika*, která umožňuje přístup k databázi pouze z určitých podů. Síťová politika je doporučena ve všech výše uvedených případech a **silně doporučena**, pokud byl nastaven přístup zvenčí Kubernetes clusteru. Viz [Network Policy](../kubernetes/security) pro obecné koncepty.

Měli byste omezit přístup buď z konkrétního Podu, nebo z konkrétní IP adresy. Nicméně, přístup z `cloudnativegp` namespace musí být povolen, aby operátor spravující Postgres cluster mohl spravovat instanci. 

#### Access Only from Specific Namespace Allowed

Tento příklad umožňuje přístup pouze z konkrétního namespace Kubernetes clusteru ([stáhnout zde](/examples/ceritsc/manifests/netpolicy-internal.yaml)). Uveďte název clusteru (pokud byl změněn) jako `spec.podSelector.matchLabels.cnpg.io/cluster` a nahraďte `[namespace]` názvem namespace, kde je postgres nasazen. Další namespace jsou povoleny přidáním více položek `namespaceSelector` do seznamu.

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: cnpg-network-policy
spec:
  podSelector:
    matchLabels:
      cnpg.io/cluster: test-cluster
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: cloudnativepg
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: [namespace]
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: [any_other_namespace]
    ...
```

#### Access Only from Specific External IP Address Allowed

Tento příklad umožňuje přístup pouze z konkrétní externí IP adresy ([stáhnout zde](/examples/ceritsc/manifests/netpolicy-external.yaml)). Uveďte název clusteru (pokud byl změněn) jako `spec.podSelector.matchLabels.cnpg.io/cluster`, nahraďte `[namespace]` názvem namespace, kde je cluster nasazen, a nahraďte `[IP]/32` vaší externí IP `/32` (tato část `slash 32` musí být poskytnuta jako koncová část adresy). Další IP bloky mohou být povoleny přidáním celé části `ipBlock` do yaml.

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: cnpg-network-policy
spec:
  podSelector:
    matchLabels:
      cnpg.io/cluster: test-cluster
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: cloudnativepg
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: [namespace]
    - ipBlock:
        cidr: [IP]/32
    - ipBlock:
        cidr: [another_IP]/32
```

## Data Backups

Postgres operátor nabízí automatické zálohy do S3 úložiště implementované prostřednictvím periodického wal streamingu a pravidelných záloh (v podstatě ekvivalent `pgdump`) pomocí `CronJobs`.

Nejprve potřebujete získat účet pro S3 úložiště, např. na [DU Cesnet](https://du.cesnet.cz/en/navody/object_storage/cesnet_s3/start), konkrétně jsou potřeba `KEY-ID` a `SECRET-ID`. Pomocí těchto dvou ID můžete vytvořit objekt `Secret` prostřednictvím [šablony](/examples/ceritsc/manifests/secret-cn.yaml), přičemž `KEY-ID` a `SECRET-ID` nahraďte skutečnými hodnotami získanými z účtu S3. 

Pro povolení zálohy přidejte sekci `backup` do manifestu clusteru. Viz [šablona](/examples/ceritsc/manifests/minimal-cn-backup.yaml), poslední sekce `backup`.

```yaml
...
  backup:
    barmanObjectStore:
      destinationPath: "s3://[BUCKET]"
      endpointURL: "https://[S3URL]"
      s3Credentials:
        accessKeyId:
          name: aws-creds
          key: ACCESS_KEY_ID
        secretAccessKey:
          name: aws-creds
          key: ACCESS_SECRET_KEY
    retentionPolicy: "30d"
```

Nahraďte `[BUCKET]` a `[S3URL]` skutečnými hodnotami. **Nehrajte si s `ACCESS_KEY_ID` a `ACCESS_SECRET_KEY`, musí odpovídat klíčovým slovům Secret, která identifikují jejich hodnoty.**

### Full Backups

Pro plné zálohy musí být vytvořeny další zdroje. 

```yaml
apiVersion: postgresql.cnpg.io/v1
kind: ScheduledBackup
metadata:
  name: [backup-name]
spec:
  schedule: "0 0 0 * * *"
  backupOwnerReference: self
  cluster:
    name: [db-name]
```

Nahraďte `[backup-name]` vhodným názvem zdroje (musí být jedinečný v rámci namespace) a `[db-name]` názvem databázového clusteru z hlavního manifestu, tyto dva názvy musí odpovídat. Např. manifest clusteru začínající:
```yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: test-cluster
```

Potřebuje manifest zálohy:
```yaml
apiVersion: postgresql.cnpg.io/v1
kind: ScheduledBackup
metadata:
  name: backup
spec:
  schedule: "0 0 0 * * *"
  backupOwnerReference: self
  cluster:
    name: test-cluster
```

Harmonogram je **NE** standardní `cron` harmonogram - tato sekunda/minuta/hodina/den v měsíci/měsíc/den v týdnu.

## High Availability Cluster Setup

Replikovaný cluster může být využit pro případy vysoké dostupnosti. Od verze Postgres 11 operátor podporuje další nastavení pro zvýšení vysoké dostupnosti.


1. Zvyšte historii segmentů `wal` pomocí následujícího úryvku. Změňte hodnotu `wal_keep_size` podle potřeby. Výchozí hodnota je přibližně 500MB, což může být málo. Tato hodnota je však alokována z podkladového úložiště. Pokud je použit `zfs-csi`, vynucuje velikost disku a DB `wals` mohou snadno spotřebovat veškeré místo na disku.
```yaml
postgresql:
    parameters:
      wal_keep_size: 64GB
```

2. Povolte možnost vysoké dostupnosti:
```yaml
replicationSlots:
    highAvailability:
      enabled: true
```

Mějte na paměti, že tyto možnosti fungují pouze pro Postgres 11 a novější. Kompletní příklad clusteru s možnostmi HA:
```yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: test-cluster
spec:
  instances: 3

  imageName: 'ghcr.io/cloudnative-pg/postgresql:14.7-3'

  primaryUpdateStrategy: unsupervised

  bootstrap:
    initdb:
      database: dbname
      owner: dbowner

  postgresql:
    parameters:
      wal_keep_size: 64GB

  replicationSlots:
    highAvailability:
      enabled: true

  resources:
    requests:
      memory: "4096Mi"
      cpu: 1
    limits:
      memory: "4096Mi"
      cpu: 1

  storage:
    size: 10Gi
    storageClass: zfs-csi
```

## Lokální úložiště vs NFS

Je možné použít lokální úložiště (SSD) místo NFS nebo jakéhokoli síťového podporovaného PVC. I když není možné přímo požadovat lokální úložiště v sekci `volume`, je možné použít lokální úložiště. Můžete si stáhnout [manifest pro jednotlivou instanci](/examples/ceritsc/manifests/minimal-local-cn.yaml), který může být také použit pro instanci clusteru (nastavte požadovaný `numberOfInstances`).

V podstatě lze použít třídu úložiště `zfs-csi` pro využití lokálního úložiště. Při nastavování limitu je třeba věnovat zvláštní pozornost. Limit nelze v budoucnu zvýšit a je vynucen, nicméně je to nejrychlejší úložiště, které je nabízeno.

## Vlastní přihlašovací údaje k databázi

Pokud chcete použít vlastní přihlašovací údaje k databázi, musí být vytvořen objekt `Secret` (uložte tento yaml do `secret.yaml`)

```yaml
apiVersion: v1
kind: Secret
type: kubernetes.io/basic-auth
stringData:
  password: [password]
  username: [username]
metadata:
  name: [secret_name]
```
Spusťte `kubectl create -f secret.yaml -n [namespace_where_postgres_will_be_deployed]`.

V manifestu clusteru změňte sekci `boostrap.initdb`, aby odkazovala na nově vytvořený secret.

```yaml
...
bootstrap:
    initdb:
      database: [database-name]
      owner: [username_from_secret]
      secret:
        name: [secret_name]
```

## Egress Network Policy

Pokud chcete řídit odchozí provoz (egress), je nutné povolit konkrétní porty a ipblocks - viz následující příklad. Pokud je pro vás vhodné omezení pouze na příchozí provoz, můžete odstranit pravidla egress.

**Vezměte prosím na vědomí, že pokud chcete používat zálohy dat a také omezit egress komunikaci, možná budete muset přidat pravidlo egress, které umožní komunikaci se zálohovacím serverem!** 

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: postgres-cnpg-np
spec:
  podSelector:
    matchLabels:
      # This Network Policy is applied to our Postgres CNPG Cluster resource named "test-cluster"
      cnpg.io/cluster: test-cluster
  policyTypes:
  - Ingress
  - Egress # <---
  ingress:
    # Enables ingress Postgres port for all Pods
  - ports:
    - port: 5432
      protocol: TCP
    from:
    - podSelector: {}
    # Enables ingress communication from the Postgres CNPG Operator installed in the Kubernetes
  - ports:
    - port: 8000
      protocol: TCP
    from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: cloudnativepg
      podSelector:
        matchLabels:
          app.kubernetes.io/name: cloudnative-pg
  egress: # <---
    # Enables egress communication to the Postgres CNPG Operator installed in the Kubernetes
  - ports:
    - port: 6443
      protocol: TCP
    to:
    - ipBlock:
        cidr: 10.43.0.1/32
    - ipBlock:
        cidr: 10.16.62.14/32
    - ipBlock:
        cidr: 10.16.62.15/32
    - ipBlock:
        cidr: 10.16.62.16/32
    - ipBlock:
        cidr: 10.16.62.17/32
    - ipBlock:
        cidr: 10.16.62.18/32
```

## Upozornění

### Chyby nasazení

1. Pokud narazíte na chybu nasazení a chcete ji smazat a znovu vytvořit, **musíte** se ujistit, že běžící instance je smazána před jejím opětovným vytvořením. Pokud vytvoříte novou instanci příliš brzy, instance nebude nikdy vytvořena a budete muset použít jiné jméno.

2. Pokud během nasazení *clusteru* dojdou kvóty, budou nasazeny pouze instance v rámci kvót. Bohužel, i v tomto případě nemůžete odstranit/znovu nasadit databázi pomocí těchto nasazení.

3. Varianta lokálního SSD není odolná vůči selhání clusteru. Data mohou být v tomto případě ztracena (např. pokud je cluster obnoven ze zálohy, lokální data nemusí být dostupná). Pravidelné zálohy jsou silně doporučovány.

4. Může se stát, že replikace selže a nemůže se připojit k clusteru, v takovém případě musí být replikace a PVC odstraněny najednou pomocí `kubectl delete pod/test-cluster-1 pvc/test-cluster-1 -n [namespace]`, pokud je selhávající replikace `test-cluster-1`. Replikace bude obnovena a synchronizována se zbytkem clusteru. Její číslo však bude zvýšeno.

### Obrázky

V současnosti docker obrazy cloudnative-pg neobsahují lokalitu `cs_CZ`, takže české řazení nemůže být použito. Z tohoto důvodu jsme vytvořili dva místní obrazy: `cerit.io/cloudnative-pg/postgresql:10.23-3` a `cerit.io/cloudnative-pg/postgresql:15.0`, které obsahují české lokality.

Seznam veřejných obrazů je k dispozici [zde](https://github.com/cloudnative-pg/postgres-containers/pkgs/container/postgresql).
